{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a934948f7030"
      },
      "source": [
        "# Language Translation with Transformers\n",
        "\n",
        "For this notebook, we will be training a transformer that would be able to input a Portuguese sentence and return the English translation. This is commonly refered to as the problem of Machine Translation.\n",
        "\n",
        "## Setup and Data Handling\n",
        "\n",
        "We will install [TensorFlow Datasets](https://tensorflow.org/datasets) for loading the dataset and [TensorFlow Text](https://www.tensorflow.org/text) for text preprocessing (Tokenization)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XFG0NDRu5mYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c2bdf8-2945-4345-8384-701bbd47496b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf~=3.20.3 in /usr/local/lib/python3.10/dist-packages (3.20.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install protobuf~=3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cCvXbPkccV1"
      },
      "source": [
        "### Downloding the datatset\n",
        "\n",
        "We make use of the TensorFlow Datasets to load the [Portuguese-English translation dataset](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en) Talks Open Translation Project. This dataset contains approximately 52,000 training, 1,200 validation and 1,800 test examples.\n",
        "\n",
        "The `tf.data.Dataset` object returned by TensorFlow Datasets yields pairs of text examples, each representing a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8q9t4FmN96eN"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load(\n",
        "  'ted_hrlr_translate/pt_to_en',\n",
        "  with_info=True,\n",
        "  as_supervised=True\n",
        ")\n",
        "\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CZFAMZJyDrFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a3258b-9279-45ea-8d5a-08451bc3deea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Examples in Portuguese:\n",
            "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "mas e se estes fatores fossem ativos ?\n",
            "mas eles não tinham a curiosidade de me testar .\n",
            "\n",
            "> Examples in English:\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "but what if it were active ?\n",
            "but they did n't test for curiosity .\n"
          ]
        }
      ],
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  print('> Examples in Portuguese:')\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "\n",
        "  print()\n",
        "\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJxTd6aVnZyh"
      },
      "source": [
        "### Set up the tokenizer\n",
        "\n",
        "We will make use of a pretrained Tokenizer fine-tuned on the Portugese and English language provided by TensorFlow. The loaded model would contain two text tokenizers, one for English and one for Portuguese, both with the same methods.\n",
        "\n",
        "Of all the available methods, we would use the `tokenize` method which converts a batch of strings to a padded-batch of token IDs. This method splits punctuation, lowercases and unicode-normalizes the input before tokenizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QToMl0NanZPr"
      },
      "outputs": [],
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "  f'{model_name}.zip',\n",
        "  f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "  cache_dir='.', cache_subdir='', extract=True\n",
        ")\n",
        "tokenizers = tf.saved_model.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z_gPC5iwFXfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7018ad-0de4-4c93-dc7b-786fe6a1e428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> This is a batch of strings:\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "but what if it were active ?\n",
            "but they did n't test for curiosity .\n",
            "\n",
            "> This is a padded-batch of token IDs:\n",
            "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
            "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
            "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
          ]
        }
      ],
      "source": [
        "print('> This is a batch of strings:')\n",
        "for en in en_examples.numpy():\n",
        "  print(en.decode('utf-8'))\n",
        "\n",
        "encoded = tokenizers.en.tokenize(en_examples)\n",
        "print()\n",
        "\n",
        "print('> This is a padded-batch of token IDs:')\n",
        "for row in encoded.to_list():\n",
        "  print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yb35sTJcZq9"
      },
      "source": [
        "### Set up a data pipeline with `tf.data`\n",
        "\n",
        "The following function takes batches of text as input, and converts them to a format suitable for training.\n",
        "\n",
        "1. It trims each to be no longer than `MAX_TOKENS`.\n",
        "2. It splits the target (English) tokens into inputs and labels. These are shifted by one step so that at each input location the `label` is the id of the next token.\n",
        "3. It converts the `RaggedTensor`s to padded dense `Tensor`s.\n",
        "4. It returns an `(inputs, labels)` pair.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6shgzEck3FiV"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS=128\n",
        "\n",
        "def prepare_batch(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    pt = pt[:, :MAX_TOKENS]             # Trim to MAX_TOKENS.\n",
        "    pt = pt.to_tensor()                 # Convert to 0-padded dense Tensor\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en[:, :(MAX_TOKENS+1)]\n",
        "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return (pt, en_inputs), en_labels\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "    ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BSswr5TKvoNM"
      },
      "outputs": [],
      "source": [
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Transformer Architecture\n",
        "\n",
        "### Word Embedding and Positional Encoding\n",
        "\n",
        "We will add a \"Positional Encoding\" to the embedding vectors. It uses a set of sines and cosines at different frequencies (across the sequence). By definition nearby elements will have similar position encodings.\n",
        "\n",
        "The original paper uses the following formula for calculating the positional encoding:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "\n",
        "- The code below implements it, but instead of interleaving the sines and cosines, the vectors of sines and cosines are simply concatenated. Permuting the channels like this is functionally equivalent, and just a little easier to implement and show in the plots below.\n",
        "- The position encoding function is a stack of sines and cosines that vibrate at different frequencies depending on their location along the depth of the embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1Rz82wEs5biZ"
      },
      "outputs": [],
      "source": [
        "# length :=  Number of tokens in the input sequence\n",
        "# depth  :=  Paramter of the model used to the Sinusoidal Position Encoding\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "    [\n",
        "      np.sin(angle_rads),\n",
        "      np.cos(angle_rads)\n",
        "    ],\n",
        "    axis=-1\n",
        "  )\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "838tmM1cm9cB"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "      vocab_size,\n",
        "      # Dimensions of the Embedding\n",
        "      d_model,\n",
        "      # Tokenizer reserves the 0 index of padding\n",
        "      mask_zero=True\n",
        "    )\n",
        "    self.pos_encoding = positional_encoding(\n",
        "      length=2048,\n",
        "      depth=d_model\n",
        "    )\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    # The length of the inputs\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE9cEBWCMKOP"
      },
      "source": [
        "### Multi Head Attention Layer\n",
        "\n",
        "Attention layers are used throughout the model. These are all identical except for how the attention is configured (it is masked in the case of the decoder and unmasked in the case of encoder). Each one contains a `layers.MultiHeadAttention`, a `layers.LayerNormalization` and a `layers.Add`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5VLa5QcdPpv5"
      },
      "outputs": [],
      "source": [
        "# A base class for attention layer + Add & Norm\n",
        "# This wil be used to create two classes: one for Masked Attention, other for Unmasked Attention\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kfHVbJUWv8qp"
      },
      "outputs": [],
      "source": [
        "# Used in Decoder\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output = self.mha(\n",
        "      query=x,\n",
        "      key=context,\n",
        "      value=context,\n",
        "    )\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4MMQ-AfKD99_"
      },
      "outputs": [],
      "source": [
        "# Used in the Decoder Layer\n",
        "# Since this layer needs to be undirectional (i.e. masking the future tokens), we would set use_causal_mask = true\n",
        "# In training, it computes the loss for each of the next positions as well, but in inference only a single token is generated\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RNqoTpn1wB3i"
      },
      "outputs": [],
      "source": [
        "# Used in Encoder\n",
        "# The flow of data in bidirectional, and thus Q,K and V are all initialized with the same vector\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x\n",
        "    )\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLjScSWQv9M5"
      },
      "source": [
        "### Feed Forward Network\n",
        "The network consists of two linear layers (`tf.keras.layers.Dense`) with a ReLU activation in-between, and a dropout layer. As with the attention layers the code here also includes the residual connection and normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rAYLeu0uwXYK"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### Encoder layer and Encoder\n",
        "\n",
        "The encoder contains a stack of `N` encoder layers, where each `EncoderLayer` contains a `GlobalSelfAttention` and `FeedForward` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ncyS-Ms3i2x_"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jpEox7gJ8FCI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(\n",
        "    self,\n",
        "    *,\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    vocab_size,\n",
        "    dropout_rate=0.1\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(\n",
        "          d_model=d_model,\n",
        "          num_heads=num_heads,\n",
        "          dff=dff,\n",
        "          dropout_rate=dropout_rate\n",
        "        ) for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # x is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape (batch_size, seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape (batch_size, seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### Decoder Layer and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9SoX0-vd1hue"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(\n",
        "    self,\n",
        "    *,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    dropout_rate=0.1\n",
        "  ):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "      num_heads=num_heads,\n",
        "      key_dim=d_model,\n",
        "      dropout=dropout_rate\n",
        "    )\n",
        "    self.cross_attention = CrossAttention(\n",
        "      num_heads=num_heads,\n",
        "      key_dim=d_model,\n",
        "      dropout=dropout_rate\n",
        "    )\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "    x = self.ffn(x)  # Shape (batch_size, seq_len, d_model)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "d5_d5-PLQXwY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(\n",
        "    self,\n",
        "    *,\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    vocab_size,\n",
        "    dropout_rate=0.1\n",
        "  ):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(\n",
        "          d_model=d_model,\n",
        "          num_heads=num_heads,\n",
        "          dff=dff,\n",
        "          dropout_rate=dropout_rate\n",
        "        ) for _ in range(num_layers)\n",
        "      ]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # x is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x = self.dropout(x)\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "The `Transformer` model consists of the Encoder, Decoder, and a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities.\n",
        "\n",
        "The output of the decoder is the input to this final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        num_layers=num_layers,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        vocab_size=input_vocab_size,\n",
        "        dropout_rate=dropout_rate\n",
        "      )\n",
        "\n",
        "    self.decoder = Decoder(\n",
        "        num_layers=num_layers,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        vocab_size=target_vocab_size,\n",
        "        dropout_rate=dropout_rate\n",
        "      )\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Context is the input that we are running the model on\n",
        "    # x is the decoded sentence till now\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)\n",
        "    x = self.decoder(x, context)\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mzyo6KDfVyhl"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "transformer = Transformer(\n",
        "  num_layers=num_layers,\n",
        "  d_model=d_model,\n",
        "  num_heads=num_heads,\n",
        "  dff=dff,\n",
        "  input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "  target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "  dropout_rate=dropout_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6y7rNP5lzd6"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss. Use the cross-entropy loss function (`tf.keras.losses.SparseCategoricalCrossentropy`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        "  )\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Una1v0hDlIsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18905317-6ac0-4a71-f91b-b15bfe8fdeff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "810/810 [==============================] - 259s 266ms/step - loss: 6.2735 - masked_accuracy: 0.0473 - val_loss: 6.2211 - val_masked_accuracy: 0.0460\n",
            "Epoch 2/25\n",
            "810/810 [==============================] - 199s 245ms/step - loss: 6.2200 - masked_accuracy: 0.0505 - val_loss: 6.4044 - val_masked_accuracy: 0.0492\n",
            "Epoch 3/25\n",
            "810/810 [==============================] - 198s 244ms/step - loss: 6.2035 - masked_accuracy: 0.0566 - val_loss: 6.7234 - val_masked_accuracy: 0.0494\n",
            "Epoch 4/25\n",
            "810/810 [==============================] - 197s 243ms/step - loss: 6.1966 - masked_accuracy: 0.0577 - val_loss: 6.7192 - val_masked_accuracy: 0.0492\n",
            "Epoch 5/25\n",
            "810/810 [==============================] - 196s 242ms/step - loss: 6.1927 - masked_accuracy: 0.0585 - val_loss: 6.9007 - val_masked_accuracy: 0.0489\n",
            "Epoch 6/25\n",
            "810/810 [==============================] - 194s 240ms/step - loss: 6.1467 - masked_accuracy: 0.0653 - val_loss: 7.2000 - val_masked_accuracy: 6.7566e-04\n",
            "Epoch 7/25\n",
            "810/810 [==============================] - 196s 241ms/step - loss: 6.0917 - masked_accuracy: 0.0731 - val_loss: 7.4772 - val_masked_accuracy: 7.1694e-04\n",
            "Epoch 8/25\n",
            "810/810 [==============================] - 197s 243ms/step - loss: 6.0804 - masked_accuracy: 0.0749 - val_loss: 8.0130 - val_masked_accuracy: 6.7841e-04\n",
            "Epoch 9/25\n",
            "810/810 [==============================] - 195s 241ms/step - loss: 6.0739 - masked_accuracy: 0.0756 - val_loss: 7.8308 - val_masked_accuracy: 7.0913e-04\n",
            "Epoch 10/25\n",
            "810/810 [==============================] - 196s 241ms/step - loss: 6.0730 - masked_accuracy: 0.0758 - val_loss: 7.9998 - val_masked_accuracy: 6.8348e-04\n",
            "Epoch 11/25\n",
            "810/810 [==============================] - 196s 241ms/step - loss: 6.0631 - masked_accuracy: 0.0777 - val_loss: 7.7655 - val_masked_accuracy: 6.8753e-04\n",
            "Epoch 12/25\n",
            "810/810 [==============================] - 195s 240ms/step - loss: 6.0555 - masked_accuracy: 0.0788 - val_loss: 8.4595 - val_masked_accuracy: 7.2887e-04\n",
            "Epoch 13/25\n",
            "810/810 [==============================] - 195s 240ms/step - loss: 6.0527 - masked_accuracy: 0.0793 - val_loss: 8.5780 - val_masked_accuracy: 7.1677e-04\n",
            "Epoch 14/25\n",
            "810/810 [==============================] - 195s 240ms/step - loss: 6.0474 - masked_accuracy: 0.0801 - val_loss: 8.5242 - val_masked_accuracy: 7.0777e-04\n",
            "Epoch 15/25\n",
            "810/810 [==============================] - 197s 242ms/step - loss: 6.0448 - masked_accuracy: 0.0808 - val_loss: 9.0153 - val_masked_accuracy: 6.8800e-04\n",
            "Epoch 16/25\n",
            "810/810 [==============================] - 194s 238ms/step - loss: 6.0410 - masked_accuracy: 0.0811 - val_loss: 8.4573 - val_masked_accuracy: 7.2644e-04\n",
            "Epoch 17/25\n",
            "810/810 [==============================] - 194s 239ms/step - loss: 6.0386 - masked_accuracy: 0.0814 - val_loss: 8.5847 - val_masked_accuracy: 7.0444e-04\n",
            "Epoch 18/25\n",
            "810/810 [==============================] - 196s 241ms/step - loss: 6.0370 - masked_accuracy: 0.0816 - val_loss: 8.4619 - val_masked_accuracy: 7.2932e-04\n",
            "Epoch 19/25\n",
            "810/810 [==============================] - 195s 240ms/step - loss: 6.0339 - masked_accuracy: 0.0823 - val_loss: 8.5836 - val_masked_accuracy: 7.1468e-04\n",
            "Epoch 20/25\n",
            "810/810 [==============================] - 196s 242ms/step - loss: 6.0301 - masked_accuracy: 0.0824 - val_loss: 8.6864 - val_masked_accuracy: 7.2047e-04\n",
            "Epoch 21/25\n",
            "810/810 [==============================] - 196s 241ms/step - loss: 6.0272 - masked_accuracy: 0.0824 - val_loss: 8.7409 - val_masked_accuracy: 6.7643e-04\n",
            "Epoch 22/25\n",
            "810/810 [==============================] - 196s 241ms/step - loss: 6.0238 - masked_accuracy: 0.0831 - val_loss: 8.5615 - val_masked_accuracy: 6.9397e-04\n",
            "Epoch 23/25\n",
            "810/810 [==============================] - 194s 239ms/step - loss: 6.0216 - masked_accuracy: 0.0830 - val_loss: 8.7051 - val_masked_accuracy: 7.1988e-04\n",
            "Epoch 24/25\n",
            "810/810 [==============================] - 196s 241ms/step - loss: 6.0202 - masked_accuracy: 0.0835 - val_loss: 9.1786 - val_masked_accuracy: 7.2101e-04\n",
            "Epoch 25/25\n",
            "810/810 [==============================] - 195s 240ms/step - loss: 6.0175 - masked_accuracy: 0.0840 - val_loss: 9.0944 - val_masked_accuracy: 6.9128e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x780ce97bf970>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "transformer.compile(\n",
        "  loss=masked_loss,\n",
        "  optimizer=\"adam\",\n",
        "  metrics=[masked_accuracy]\n",
        ")\n",
        "\n",
        "transformer.fit(\n",
        "  train_batches,\n",
        "  epochs=25,\n",
        "  validation_data=val_batches\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxKpqCbzSW6z"
      },
      "source": [
        "## Inference\n",
        "\n",
        "We now test the model by performing a translation. The following steps are used for inference:\n",
        "\n",
        "* Encode the input sentence using the Portuguese tokenizer (`tokenizers.pt`). This is the encoder input.\n",
        "* The decoder input is initialized to the `[START]` token.\n",
        "* Calculate the padding masks and the look ahead masks.\n",
        "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
        "* Concatenate the predicted token to the decoder input and pass it to the decoder.\n",
        "* In this approach, the decoder predicts the next token based on the previous tokens it predicted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eY_uXsOhSmbb"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is English, initialize the output with the English `[START]` token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]              # Shape (batch_size, 1, vocab_size)\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is (1, tokens)\n",
        "    text = tokenizers.en.detokenize(output)[0]\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    return text, tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-NjbvpHUTEia"
      },
      "outputs": [],
      "source": [
        "translator = Translator(tokenizers, transformer)\n",
        "\n",
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "o9CEm4cuTGtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f181c3-e939-40e5-9641-0fc6b3ac9959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : este é um problema que temos que resolver.\n",
            "Prediction     : thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank\n",
            "Ground truth   : this is a problem we have to solve .\n"
          ]
        }
      ],
      "source": [
        "# Example 1\n",
        "sentence = 'este é um problema que temos que resolver.'\n",
        "ground_truth = 'this is a problem we have to solve .'\n",
        "\n",
        "translated_text, translated_tokens = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "elmz_Ly7THuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49d7bd3-608a-4046-e05d-883fc455b1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : os meus vizinhos ouviram sobre esta ideia.\n",
            "Prediction     : thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank\n",
            "Ground truth   : and my neighboring homes heard about this idea .\n"
          ]
        }
      ],
      "source": [
        "# Example 2\n",
        "sentence = 'os meus vizinhos ouviram sobre esta ideia.'\n",
        "ground_truth = 'and my neighboring homes heard about this idea .'\n",
        "\n",
        "translated_text, translated_tokens = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bmmtPo3vTOwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8b0e57-b40d-4a4a-f24e-159eb4be3213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
            "Prediction     : thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank thank\n",
            "Ground truth   : so i'll just share with you some stories very quickly of some magical things that have happened.\n"
          ]
        }
      ],
      "source": [
        "# Example 3\n",
        "sentence = 'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'\n",
        "ground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n",
        "\n",
        "translated_text, translated_tokens = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}