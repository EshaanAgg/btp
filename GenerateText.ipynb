{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text Generation with an RNN\n",
        "\n",
        "This notebook demonstrates how to generate text using a character-based RNN. We will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
        "\n",
        "In the final results of the notebook you can see that while some of the sentences are grammatical, and only make partial sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based.\n",
        "* The structure of the output resembles a playâ€”blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "* The model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.\n",
        "* The model has no concept of words as of now (as there are no embeddings used), but still it is able to train it's own character embeddings and produce actual words for the most part. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "## Setup and Preprocessing\n",
        "\n",
        "To train the model in a reasonable time and with a relatively smaller training corpus, we would we would be training the model to predict the next character according to the given contetual window, instead of training it on words and their embeddings.\n",
        "\n",
        "We would define the following utilities:\n",
        "- `process_text`: This function reads the file with `file_path` destination and returns a mapping of characters to index, index to character, and all the text encoded as indices in an array.\n",
        "- `split_input_target`: It creates a (training data, output) pair for each fixed length sentence that we provide to it.\n",
        "- `create_dataset`: It takes in a file and then converts it to a dataset by batching the sentences (sequences) that are fed into the model. This also allows for the shuffling of data, improving the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nJwf0QU57wtD"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-16T12:28:51.777470Z",
          "iopub.status.busy": "2023-11-16T12:28:51.777245Z",
          "iopub.status.idle": "2023-11-16T12:28:54.138340Z",
          "shell.execute_reply": "2023-11-16T12:28:54.137637Z"
        },
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "def process_text(file_path):\n",
        "    text = open(file_path, 'rb').read().decode(encoding='utf-8')\n",
        "    vocab = sorted(set(text))  # The unique characters in the file\n",
        "\n",
        "    # Creating a mapping from unique characters to indices and vice versa\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "    return text_as_int, vocab, char2idx, idx2char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RbVvaPUv6hfx"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text, target_text = chunk[:-1], chunk[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0SbFRnan6i6E"
      },
      "outputs": [],
      "source": [
        "def create_dataset(text_as_int, seq_length, batch_size, buffer_size):\n",
        "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "    # Create sequences and map then to a pair of input and output\n",
        "    dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\n",
        "    # Create batches of the dataset and shuffle them in memory in `buffer_size` intervals\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TAYVeKZ8orJ"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "The model is defined as a Sequential model in TensorFlow Keras, indicating a linear stack of layers.\n",
        "\n",
        "1. **Embedding Layer:**\n",
        "    - Purpose: Converts integer-encoded vocabulary indices into dense vectors of fixed size.\n",
        "    - Parameters:\n",
        "        - `vocab_size`: Size of the vocabulary, i.e., the total number of unique words in the input.\n",
        "        - `embedding_dim`: Dimension of the dense embedding.\n",
        "        - `batch_input_shape`: Shape of the input data, with `None` indicating variable sequence length.\n",
        "        - `trainable`: This option allows the encoding of the character to a higher dimensional geometry representing their semantic meaning.\n",
        "\n",
        "2. **LSTM Layer**\n",
        "    - Purpose: Long Short-Term Memory (LSTM) layer with return sequences set to True, indicating it returns the full sequence of outputs for each input sequence.\n",
        "    - Parameters:\n",
        "        - `rnn_units`: Number of LSTM units.\n",
        "        - `return_sequences`: True to return the full sequence.\n",
        "        - `stateful`: True to maintain state across batches.\n",
        "        - `recurrent_initializer`: Initialization for the recurrent weights.\n",
        "\n",
        "3. **Dropout Layer**\n",
        "    - Purpose: Introduces dropout to prevent overfitting during training.\n",
        "    - Parameter:\n",
        "        - `0.1`: Fraction of input units to drop.\n",
        "\n",
        "4. **Batch Normalization Layer**\n",
        "    - Purpose: Normalizes and scales the inputs, helping stabilize and accelerate the training process.\n",
        "\n",
        "5. **Dense Output Layer:**\n",
        "    - Purpose: Dense (fully connected) layer responsible for generating the output predictions.\n",
        "    - Parameter:\n",
        "        - `vocab_size`: Number of units, representing the size of the output vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "m_IeMMQc6jGG"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None], trainable=True),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        keras.layers.Dropout(0.1),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        keras.layers.Dropout(0.1),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GGuGa1yq6o3t"
      },
      "outputs": [],
      "source": [
        "# Definning a loss function with logbits enabled\n",
        "# This would be used in the training of our model\n",
        "def loss(labels, logits):\n",
        "    return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "THSUju1w6qlI"
      },
      "outputs": [],
      "source": [
        "# Utility to generate the text\n",
        "def generate_text(model, char2idx, idx2char, start_string, generate_char_num, temperature=1.0):\n",
        "    # Low temperatures results in more predictable text, higher temperatures results in more surprising text.\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "\n",
        "    # Expand the dimension of the input by 1 as the model expects batches\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "    model.reset_states()\n",
        "\n",
        "    for _ in range(generate_char_num):\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        # Remove the extra dimension correspinding to the batches in the output\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        predictions /= temperature\n",
        "        # Using a categorical distribution to predict the character returned by the model\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], axis=0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdJkJ_BICGkZ"
      },
      "source": [
        "## Running the model\n",
        "\n",
        "We have use of two models for simplicity.\n",
        "- We first train a model which can accept batches of text (of size `64`) for better performance and training on the supplied data. When the training of the same is complete, we export the weights of the same.\n",
        "- This model is never used for the actual prediction.\n",
        "- Then we create another model with exactly the same architecture as the previous model, but with a `batch_size` of `1`. This allows this model to reuse the weights of the previously trained model, but allows us to pass the inputs simply as an array, without having to worry about batching them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uWzUo9m6syw",
        "outputId": "943ab50a-f576-4859-81b5-f856cd957095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (64, None, 1024)          0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (64, None, 1024)          4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (64, None, 1024)          0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (64, None, 1024)          4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13731137 (52.38 MB)\n",
            "Trainable params: 13727041 (52.36 MB)\n",
            "Non-trainable params: 4096 (16.00 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "172/172 [==============================] - 36s 176ms/step - loss: 2.3568\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 33s 180ms/step - loss: 1.6511\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 32s 177ms/step - loss: 1.5059\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 32s 176ms/step - loss: 1.4334\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 33s 177ms/step - loss: 1.3827\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 32s 177ms/step - loss: 1.3424\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 32s 176ms/step - loss: 1.3073\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 32s 177ms/step - loss: 1.2740\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 32s 176ms/step - loss: 1.2395\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 33s 177ms/step - loss: 1.2049\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 32s 177ms/step - loss: 1.1665\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 33s 180ms/step - loss: 1.1256\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 34s 185ms/step - loss: 1.0823\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 33s 180ms/step - loss: 1.0348\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 0.9864\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 33s 181ms/step - loss: 0.9359\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 33s 180ms/step - loss: 0.8837\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 33s 179ms/step - loss: 0.8355\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 34s 179ms/step - loss: 0.7881\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 33s 179ms/step - loss: 0.7438\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 32s 177ms/step - loss: 0.7009\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 33s 176ms/step - loss: 0.6648\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 32s 175ms/step - loss: 0.6325\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.6020\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 0.5738\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 0.5530\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 33s 183ms/step - loss: 0.5310\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 34s 180ms/step - loss: 0.5140\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 33s 178ms/step - loss: 0.4974\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 32s 178ms/step - loss: 0.4841\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 33s 178ms/step - loss: 0.4723\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 32s 179ms/step - loss: 0.4600\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 33s 179ms/step - loss: 0.4502\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 33s 179ms/step - loss: 0.4417\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 32s 179ms/step - loss: 0.4339\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 34s 187ms/step - loss: 0.4268\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 0.4188\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 33s 183ms/step - loss: 0.4133\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 33s 181ms/step - loss: 0.4083\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 34s 182ms/step - loss: 0.4034\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 33s 181ms/step - loss: 0.3971\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 33s 183ms/step - loss: 0.3924\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 33s 182ms/step - loss: 0.3895\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 32s 179ms/step - loss: 0.3850\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 33s 180ms/step - loss: 0.3796\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 33s 179ms/step - loss: 0.3774\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 32s 177ms/step - loss: 0.3751\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 32s 175ms/step - loss: 0.3706\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 32s 175ms/step - loss: 0.3692\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 33s 174ms/step - loss: 0.3644\n"
          ]
        }
      ],
      "source": [
        "# Load the standard `shakespeare` text file provided by TensorFlow\n",
        "path_to_file = keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Define the constants\n",
        "RNN_UNITS = 1024\n",
        "EMBEDDING_DIMENSIONS = 256\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "SEQUENCE_LENGTH=100\n",
        "TEXT_TO_GENERATE=2000\n",
        "EPOCHS = 50\n",
        "\n",
        "# Processing the file\n",
        "text_as_int, vocab, char2idx, idx2char = process_text(path_to_file)\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "# Create the dataset\n",
        "dataset = create_dataset(\n",
        "  text_as_int,\n",
        "  SEQUENCE_LENGTH,\n",
        "  BATCH_SIZE,\n",
        "  BUFFER_SIZE\n",
        ")\n",
        "# Create the model\n",
        "model = build_model(\n",
        "  VOCAB_SIZE,\n",
        "  EMBEDDING_DIMENSIONS,\n",
        "  RNN_UNITS,\n",
        "  BATCH_SIZE\n",
        ")\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "model.summary()\n",
        "# Train the model on the dataset and save the results\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n",
        "model.save_weights(\"shakespeare_weights.h5\", save_format='h5')\n",
        "\n",
        "# Create the prediction model but with batch_size = 1\n",
        "model = build_model(\n",
        "  VOCAB_SIZE,\n",
        "  EMBEDDING_DIMENSIONS,\n",
        "  RNN_UNITS,\n",
        "  1\n",
        ")\n",
        "model.load_weights(\"shakespeare_weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETxqwzCK6yyE",
        "outputId": "37258e93-e42f-42d3-a5d4-86248b727abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KING HENRY VI: You shall face death.\n",
            "\n",
            "BRUTUS:\n",
            "But since you shall perceive your grace my sons should proud te as the posterns\n",
            "So excellent feathers from the earth to see him such a cuff\n",
            "Than a paper early to the seat,\n",
            "And tunder'd my command.\n",
            "\n",
            "WARWICK:\n",
            "I will be mild: I know you all short as sweet\n",
            "That you take with unthat which grieves my heart,\n",
            "And wet my choice is now my meaning to return.\n",
            "Perchance she cannot meet him: that's the matter?\n",
            "\n",
            "Messenger:\n",
            "The news,\n",
            "Ah, my young prince, whose honourable thoughts,\n",
            "Though these were honour!\n",
            "\n",
            "MERCUTIO:\n",
            "O heavens! what fear shall wheelike him down and in the chair of state,\n",
            "My name the lights, I woo not\n",
            "Our renowned spirit.\n",
            "\n",
            "Provost:\n",
            "Here, my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "No more of this. Canst thou tell if\n",
            "Claudio is condemned.\n",
            "\n",
            "LEONTES:\n",
            "As now she proved the liars.\n",
            "\n",
            "MENENIUS:\n",
            "O sir, you are not; 'tis it true.\n",
            "\n",
            "POMPEY:\n",
            "If you head at his pomp,\n",
            "Allowing him a breath, a little dinear, the sport is done effect.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Well, bear you where you should bestrew him o'er 'tway the bening\n",
            "of his highness and mistress of his charge,\n",
            "And there so mister, if I could have been so tedious;\n",
            "His daughter is more best, only to be thanks,\n",
            "As then the sea tode better\n",
            "\n",
            "ISABELLA:\n",
            "Be ready, as your lives shall not come I'll speak,\n",
            "'Tis more than we do sun, their births garicansw to slaughter thee.\n",
            "\n",
            "LADY GREY:\n",
            "'Twill griev the time of day unto the houst;\n",
            "And sometime was better to his tinder?\n",
            "\n",
            "First Musician:\n",
            "What advise you, gentle made!\n",
            "I am a man.\n",
            "\n",
            "MENENIUS:\n",
            "Hail to your bosom's truth, and belock soonhoode\n",
            "Of never have an unking a sword as the excellent,\n",
            "If it be husbanded with modesty.\n",
            "\n",
            "First Huntsman:\n",
            "My lord, I warrant you, and not of Tunis.\n",
            "\n",
            "GONZALO:\n",
            "This Tunis, sir, was Carthage.\n",
            "\n",
            "ADRIAN:\n",
            "Carthage?\n",
            "\n",
            "GONZALO:\n",
            "I assure you, Camillo,; be good to you into\n",
            "He shall not far revenge this widowhom:\n",
            "We can my fair life, I here commends\n",
            "The contrary are in a read overhage it to tread o'er this done.\n",
            "\n",
            "PAULINA:\n",
            "Not so:\n",
            "I am as friend voice. You denied to me,\n",
            "So ease the more than e\n",
            "RUNTIME:  34.6554114818573\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Predict the text and generate content\n",
        "user_input = \"KING HENRY VI: You shall face death.\"\n",
        "\n",
        "start = time.time()\n",
        "generated_text = generate_text(model, char2idx, idx2char, user_input, TEXT_TO_GENERATE)\n",
        "end = time.time()\n",
        "\n",
        "print(generated_text)\n",
        "print(\"RUNTIME: \", end - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
